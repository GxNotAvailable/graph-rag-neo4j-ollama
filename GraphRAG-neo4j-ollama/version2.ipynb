{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "BM25+向量检索\n",
    "知识库动态更新函数\n",
    "一个reranker(使用了CrossEncoder)\n",
    "增加对输入的query的处理\n",
    "进一步增加易用性：1.提示词优化  2.memory\n",
    "\n",
    "COT?"
   ],
   "id": "4234196630c4e3f4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T01:45:34.974986Z",
     "start_time": "2025-04-26T01:45:34.485050Z"
    }
   },
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm = OllamaFunctions(model=\"qwen2.5\", temperature=0, format=\"json\") #可以替换成llama3\n",
    "\n",
    "reranker = 'BAAI/bge-reranker-base'\n",
    "# reranker = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "\n",
    "# # jina-embeddings-v3\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# class HuggingfaceEmbedding:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = SentenceTransformer(model, trust_remote_code=True)\n",
    "#         self.model = self.model.to(device)\n",
    "# \n",
    "#     def __call__(self, texts):\n",
    "#         return self.model.encode(texts, device=device)\n",
    "# \n",
    "# embeddings = HuggingfaceEmbedding(\"jinaai/jina-embeddings-v3\")\n",
    "\n",
    "# bge-large\n",
    "class NewOllamaEmbeddings:\n",
    "    def __init__(self, model):\n",
    "        self.model = OllamaEmbeddings(model=model)\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        return self.model.embed_query(texts)\n",
    "\n",
    "Embedding_Model = \"bge-large\"\n",
    "embeddings = NewOllamaEmbeddings(\n",
    "            model=Embedding_Model,\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 2\n",
      "Python-dotenv could not parse statement starting at line 3\n",
      "Python-dotenv could not parse statement starting at line 4\n",
      "Python-dotenv could not parse statement starting at line 5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T01:45:35.036978Z",
     "start_time": "2025-04-26T01:45:34.983521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data\n",
    "def load_excel_data(file):\n",
    "    df = pd.read_excel(file)\n",
    "    \n",
    "    nodes = {\n",
    "        \"现象\":df[\"现象\"].dropna().unique(),\n",
    "        \"原因\":df[\"原因\"].dropna().unique(),\n",
    "        \"部件\":df[\"部件\"].dropna().unique(),\n",
    "        \"处理方法\":df[\"处理方法\"].dropna().unique(),\n",
    "    }\n",
    "    \n",
    "    relationships = []\n",
    "    for _, row in df.iterrows():\n",
    "        relationships.append({\n",
    "            \"现象\": row[\"现象\"],\n",
    "            \"原因\": row[\"原因\"],\n",
    "            \"部件\": row[\"部件\"],\n",
    "            \"处理方法\": row[\"处理方法\"],\n",
    "        })\n",
    "        \n",
    "    return nodes, relationships\n",
    "\n",
    "file_path = \"故障四元组.xlsx\"\n",
    "nodes, relationships = load_excel_data(file_path)"
   ],
   "id": "429a388d6c9b70e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T01:45:58.832680Z",
     "start_time": "2025-04-26T01:45:35.068969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 图数据库\n",
    "# create graph database\n",
    "class Neo4jGraphManager:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.embeddings = embeddings\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "        \n",
    "    def clear_database(self):\n",
    "        with self.driver.session() as session:\n",
    "            try:\n",
    "                session.run(\"DROP INDEX vector_index IF EXISTS\")\n",
    "                session.run(\"DROP INDEX text_index IF EXISTS\")\n",
    "            except Exception as e:\n",
    "                print(f\"warning when dropping index : {str(e)}\")\n",
    "            \n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            \n",
    "    def insert_node_with_embedding(self, label, properties):\n",
    "        with self.driver.session() as session:\n",
    "            embedding = self.embeddings(properties[\"text\"])\n",
    "            \n",
    "            query = f\"\"\"\n",
    "            CREATE (n:{label}{{\n",
    "                name: $name,\n",
    "                text: $text,\n",
    "                embedding: $embedding,\n",
    "                uuid: $uuid\n",
    "            }})\n",
    "            RETURN n.uuid AS node_id\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query,{\n",
    "                \"name\": properties[\"name\"],\n",
    "                \"text\": properties[\"text\"],\n",
    "                \"embedding\": embedding,\n",
    "                \"uuid\": str(uuid.uuid4())\n",
    "            })\n",
    "            \n",
    "            return result.single()[\"node_id\"]\n",
    "        \n",
    "    def insert_relationship(self, start_label, start_name, end_label, end_name, relationship_type):\n",
    "        with self.driver.session() as session:\n",
    "            \n",
    "            check_start_query = f\"\"\"\n",
    "            MATCH (a:{start_label} {{name: $start_name}})\n",
    "            RETURN count(a) > 0 AS node_exists\n",
    "            \"\"\"\n",
    "            \n",
    "            start_node_exists = session.run(check_start_query, {\"start_name\": start_name}).single()[\"node_exists\"]\n",
    "            \n",
    "            if not start_node_exists:\n",
    "                raise ValueError(f\"start node {start_label} (name={start_name}) does not exist\")\n",
    "            \n",
    "            check_end_query = f\"\"\"\n",
    "            MATCH (a:{end_label} {{name: $end_name}})\n",
    "            RETURN count(a) > 0 AS node_exists\n",
    "            \"\"\"\n",
    "            \n",
    "            end_node_exists = session.run(check_end_query, {\"end_name\": end_name}).single()[\"node_exists\"]\n",
    "            \n",
    "            if not end_node_exists:\n",
    "                raise ValueError(f\"end node {end_label} (name={end_name}) does not exist\")\n",
    "            \n",
    "            query = f\"\"\"\n",
    "            MATCH (a:{start_label} {{name: $start_name}})\n",
    "            MATCH (b:{end_label} {{name: $end_name}})\n",
    "            MERGE (a)-[r:{relationship_type}]->(b)\n",
    "            RETURN type(r) AS relationship_type\n",
    "            \"\"\"\n",
    "            result = session.run(query, {\"start_name\": start_name, \"end_name\": end_name})\n",
    "            \n",
    "            return result.single()[\"relationship_type\"]\n",
    "        \n",
    "    def create_indexes(self):\n",
    "        # create indexes to accelerate the query for \"现象\"\n",
    "        with self.driver.session() as session:\n",
    "            try:\n",
    "                session.run(\"\"\"\n",
    "                CREATE VECTOR INDEX vector_index IF NOT EXISTS\n",
    "                FOR (n:现象)\n",
    "                ON (n.embedding)\n",
    "                OPTIONS {\n",
    "                indexConfig: {\n",
    "                    `vector.dimensions`: 1024,\n",
    "                    `vector.similarity_function`: 'cosine'\n",
    "                }\n",
    "            }\n",
    "                \"\"\")\n",
    "                \n",
    "                session.run(\"CALL db.awaitIndexes(300)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Creating vector indexes failed : {str(e)}\")\n",
    "            \n",
    "            try:\n",
    "                session.run(\"\"\"\n",
    "                CREATE TEXT INDEX text_index IF NOT EXISTS\n",
    "                FOR (n:现象)\n",
    "                ON (n.text)\n",
    "                \"\"\")\n",
    "                \n",
    "                session.run(\"CALL db.awaitIndexes(300)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Creating text indexes failed : {str(e)}\")\n",
    "                \n",
    "    def verify_indexes(self):\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(\"\"\"\n",
    "            SHOW INDEXES\n",
    "            YIELD name, labelsOrTypes, properties, state\n",
    "            WHERE name IN ['vector_index', 'text_index']\n",
    "            RETURN name, state\n",
    "            \"\"\")\n",
    "            \n",
    "            indexes = {record[\"name\"]: record[\"state\"] for record in result}\n",
    "            \n",
    "            missing_indexes = []\n",
    "            if 'vector_index' not in indexes:\n",
    "                missing_indexes.append(\"Vector index\")\n",
    "            if 'text_index' not in indexes:\n",
    "                missing_indexes.append(\"Text index\")\n",
    "\n",
    "            if missing_indexes:\n",
    "                print(f\"{', '.join(missing_indexes)} missing, creating...\")\n",
    "                self.create_indexes()\n",
    "                \n",
    "            elif indexes['text_index'] != 'ONLINE':\n",
    "                print(f\"Vector index state: {indexes['vector_index']}, waiting...\")\n",
    "                session.run(\"CALL db.awaitIndexes(300)\")\n",
    "\n",
    "\n",
    "            elif indexes['vector_index'] != 'ONLINE':\n",
    "                print(f\"Vector index state: {indexes['vector_index']}, waiting...\")\n",
    "                session.run(\"CALL db.awaitIndexes(300)\")\n",
    "                \n",
    "    def retrieve_with_priority(self, query:str, k=3):\n",
    "        \n",
    "        with self.driver.session() as session:\n",
    "            print(f\"[DEBUG] Query: {query}\")\n",
    "            embedding = self.embeddings(query)\n",
    "            print(f\"[DEBUG] Generated embedding for query: {embedding[:5]}... (truncated for brevity)\")\n",
    "            \n",
    "            # contains 检索\n",
    "            graph_query = \"\"\"\n",
    "            MATCH (n)\n",
    "            WHERE n.text CONTAINS $query_text\n",
    "            RETURN n.text AS text, n.name AS name, n.uuid AS uuid\n",
    "            LIMIT $k\n",
    "            \"\"\"\n",
    "            \n",
    "            graph_results = session.run(graph_query, {\"query_text\": query, \"k\": k})\n",
    "            graph_matches = [\n",
    "                {\"uuid\": record[\"uuid\"], \"text\": record[\"text\"]}\n",
    "                for record in graph_results\n",
    "            ]\n",
    "            print(f\"[DEBUG] Graph database matches found: {len(graph_matches)}\")\n",
    "            \n",
    "            # 向量检索,余弦相似度\n",
    "            try:\n",
    "                vector_results = session.run(\"\"\"\n",
    "                MATCH (n:现象)\n",
    "                WHERE n.embedding IS NOT NULL\n",
    "                RETURN n.text AS text, n.uuid AS uuid,\n",
    "                    vector.similarity.cosine(n.embedding, $embedding) AS score\n",
    "                ORDER BY score DESC\n",
    "                LIMIT $k\n",
    "                \"\"\",{\n",
    "                    \"embedding\": embedding,\n",
    "                    \"k\": k\n",
    "                })\n",
    "                \n",
    "                vector_matches = []\n",
    "                for record in vector_results:\n",
    "                    vector_matches.append({\n",
    "                        \"uuid\" : record[\"uuid\"],\n",
    "                        \"text\" : record[\"text\"],\n",
    "                        \"score\" : record[\"score\"]\n",
    "                    })\n",
    "                print(f\"[DEBUG] Vector search matches found: {vector_matches}\")\n",
    "                print(f\"[DEBUG] Vector search matches found: {len(vector_matches)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[DEBUG] Vector search failed : {str(e)}\")\n",
    "                vector_matches = []\n",
    "                \n",
    "            # Remove the 'score' field from vector_matches\n",
    "            for item in vector_matches:\n",
    "                item.pop('score', None)\n",
    "            \n",
    "            # Create a set of UUIDs from graph_matches\n",
    "            graph_uuids = {item['uuid'] for item in graph_matches}\n",
    "            \n",
    "            # Add items from graph_matches to combined_results only if their UUID is not in vector_matches\n",
    "            combined_results = graph_matches + [\n",
    "                item for item in vector_matches if item['uuid'] not in graph_uuids\n",
    "            ]\n",
    "            \n",
    "            print(f\"[DEBUG] Combined search results: {len(combined_results)}\")\n",
    "                    \n",
    "            return combined_results\n",
    "\n",
    "# construct graph database        \n",
    "graph_manager = Neo4jGraphManager(uri=os.environ[\"NEO4J_URI\"], user=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"])\n",
    "\n",
    "graph_manager.clear_database()\n",
    "\n",
    "for label, names in nodes.items():\n",
    "    for name in names:\n",
    "        graph_manager.insert_node_with_embedding(label=label, properties={\"name\": name, \"text\": name})\n",
    "\n",
    "for rel in relationships:\n",
    "    graph_manager.insert_relationship(\"现象\", rel[\"现象\"], \"原因\", rel[\"原因\"],\"原因\")\n",
    "    graph_manager.insert_relationship(\"现象\", rel[\"现象\"], \"部件\", rel[\"部件\"],\"涉及的部件\")\n",
    "    graph_manager.insert_relationship(\"现象\", rel[\"现象\"], \"处理方法\", rel[\"处理方法\"],\"处理方法\")\n",
    "\n",
    "# create index\n",
    "graph_manager.create_indexes()\n",
    "graph_manager.verify_indexes()\n",
    "\n",
    "print(\"[DEBUG] Construction successfully finished\")\n",
    "\n",
    "graph_manager.close()\n",
    "# \n",
    "# # test retriever\n",
    "# graph_manager = Neo4jGraphManager(uri=os.environ[\"NEO4J_URI\"], user=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"])\n",
    "# print(graph_manager.retrieve_with_priority(\"天线不转动\"))\n",
    "# graph_manager.close()"
   ],
   "id": "505cb44843530bca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Construction successfully finished\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T01:47:27.427790Z",
     "start_time": "2025-04-26T01:45:58.942957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, graph_manager, llm=llm,embeddings=embeddings,reranker=reranker):\n",
    "        self.graph_manager = graph_manager\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.reranker = reranker\n",
    "        self.history = []\n",
    "    \n",
    "    # 处理询问\n",
    "    def refine_query(self, query):\n",
    "        prompt_template = \"\"\"请你从<语句>中提取<现象>，仅仅提取原话中描述故障现象的内容，不添加任何解释或改动。\n",
    "            示例：\n",
    "            <语句>：波导内积水如何处理\n",
    "            <现象>：波导内积水\n",
    "            <语句>：荧光屏不亮是怎么回事\n",
    "            <现象>：荧光屏不亮\n",
    "            <语句>：而且高压指示灯亮起\n",
    "            <现象>：高压指示灯亮起\n",
    "            \n",
    "            请从语句中提取现象：\n",
    "            <语句>：{query}\n",
    "            <现象>：\n",
    "            \"\"\"\n",
    "        prompt = prompt_template.format(query=query)\n",
    "        try:\n",
    "            # 调用LLM\n",
    "            response = self.llm.invoke(prompt)\n",
    "            # 假设LLM返回的JSON包含\"故障现象\"字段\n",
    "            refined_query = response.content  # 如果提取失败，回退到原始查询\n",
    "            print(f\"[DEBUG] Refined Query (via LLM): {refined_query}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] LLM refinement failed: {str(e)}, falling back to original query\")\n",
    "            refined_query = query  # 如果LLM调用失败，回退到原始查询\n",
    "        return refined_query\n",
    "            \n",
    "    \n",
    "    # 计算BM25分数\n",
    "    def Cal_BM25(self, query, k=5):\n",
    "        \"\"\"\n",
    "        使用 jieba 分词计算 BM25 得分\n",
    "        :param query: 查询文本 (str)\n",
    "        :param graph_manager: 图数据库接口\n",
    "        :param k: 返回前 k 个结果\n",
    "        :return: BM25 匹配结果 (list of dict)\n",
    "        \"\"\"\n",
    "        bm25_matches = []\n",
    "        with self.graph_manager.driver.session() as session:\n",
    "            all_docs = session.run(\"MATCH (n:现象) RETURN n.text AS text\")  \n",
    "            # 1. 处理文档集合\n",
    "            documents = [record[\"text\"] for record in all_docs]\n",
    "            # print(f\"[DEBUG] Documents: {documents}\")\n",
    "            \n",
    "            # 使用 jieba 分词处理文档\n",
    "            tokenized_corpus = [list(jieba.cut(doc)) for doc in documents]  # 对每个文档分词\n",
    "            # print(f\"[DEBUG] Tokenized Corpus: {tokenized_corpus}\")\n",
    "            \n",
    "            # 初始化 BM25 模型\n",
    "            bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        # 2. BM25 检索\n",
    "        try:\n",
    "            # 使用 jieba 分词处理查询\n",
    "            tokenized_query = list(jieba.cut(query))\n",
    "            # print(f\"[DEBUG] Tokenized Query: {tokenized_query}\")\n",
    "            \n",
    "            # 计算 BM25 得分\n",
    "            bm25_scores = bm25.get_scores(tokenized_query)\n",
    "            bm25_top_k = np.argsort(bm25_scores)[::-1][:k]  # 取前 k 个得分最高的文档\n",
    "            \n",
    "            # 存储匹配结果\n",
    "            for idx in bm25_top_k:\n",
    "                bm25_matches.append({\n",
    "                    'text': documents[idx],\n",
    "                    'bm25_score': bm25_scores[idx]\n",
    "                })\n",
    "                print(f\"[DEBUG] BM25 search matches found: {documents[idx]} (score: {bm25_scores[idx]:.3f})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] BM25 search failed: {str(e)}\")\n",
    "            bm25_matches = []\n",
    "        \n",
    "        return bm25_matches\n",
    "\n",
    "    # 计算向量相似度\n",
    "    def Cal_Similarity(self, query, k=5):\n",
    "        vector_matches = []\n",
    "        try:\n",
    "            query_embedding = self.embeddings(query)\n",
    "            print(f\"[DEBUG] Query embedding: {query_embedding[:5]}...(truncated for brevity)\")\n",
    "            print(f\"[DEBUG] Vector length: {len(query_embedding)}\")\n",
    "            \n",
    "            with self.graph_manager.driver.session() as session:\n",
    "                index_check = session.run(\"\"\"\n",
    "                SHOW INDEXES\n",
    "                YIELD name, labelsOrTypes, properties\n",
    "                WHERE name = 'vector_index'\n",
    "                RETURN count(*) as count\n",
    "                \"\"\")\n",
    "                \n",
    "                if index_check.single()['count'] == 0:\n",
    "                    print(f\"[DEBUG] Vector index not found, creating index...\")\n",
    "                    self.graph_manager.create_indexes()\n",
    "                \n",
    "                vector_results = session.run(\"\"\"\n",
    "                CALL db.index.vector.queryNodes('vector_index', $k, $embedding)\n",
    "                YIELD node, score\n",
    "                RETURN node.text AS text, score AS score\n",
    "                ORDER BY score DESC\n",
    "                \"\"\", {\"k\":k, \"embedding\":query_embedding})\n",
    "                \n",
    "                for record in vector_results:\n",
    "                    vector_matches.append({\n",
    "                        'text': record['text'],\n",
    "                        'vector_score': record['score']\n",
    "                    })\n",
    "                    print(f\"[DEBUG] Vector search matches found: {record['text']}(vector_score: {record['score']:.3f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[DEBUG] Vector search failed : {str(e)}\") \n",
    "            vector_matches = []\n",
    "        \n",
    "        print(f\"[DEBUG] Vector result count: {len(vector_matches)}\")\n",
    "        return vector_matches\n",
    "\n",
    "    # BM25+向量混合检索\n",
    "    def hybrid_retriever(self, query, bm25_weight=0.3, vector_weight=0.7, k=5):\n",
    "        bm25_matches = self.Cal_BM25(query,  k=k)\n",
    "        vector_matches = self.Cal_Similarity(query, k=k)\n",
    "        combined_matches = {}\n",
    "        for match in vector_matches:\n",
    "            combined_matches[match['text']] = {'vector_score': match['vector_score'], 'bm25_score': 0.0}\n",
    "        for match in bm25_matches:\n",
    "            if match['text'] in combined_matches:\n",
    "                combined_matches[match['text']]['bm25_score'] = match['bm25_score']\n",
    "            else:\n",
    "                combined_matches[match['text']] = {'vector_score': 0.0, 'bm25_score': match['bm25_score']}\n",
    "    \n",
    "        # 计算融合得分\n",
    "        for text, scores in combined_matches.items():\n",
    "            hybrid_score = bm25_weight * scores['bm25_score'] + vector_weight * scores['vector_score']\n",
    "            combined_matches[text]['hybrid_score'] = hybrid_score\n",
    "    \n",
    "        # 转换为列表并排序\n",
    "        rerank_input = [{'text': text, 'hybrid_score': scores['hybrid_score']} \n",
    "                        for text, scores in combined_matches.items()]\n",
    "        reranked_matches = sorted(rerank_input, key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return reranked_matches\n",
    "\n",
    "    # 使用cross_encoder重排序\n",
    "    def rerank_with_cross_encoder(self, query: str, matches: list, top_k: int = 5, alpha: int = 0.99, beta : int = 0.01) -> list:\n",
    "        \"\"\"\n",
    "        使用中文优化的 Cross-Encoder 对混合检索结果进行重排序\n",
    "        :param query: 查询文本 (中文)\n",
    "        :param matches: 混合检索结果 (list of dict, 包含 'text' 和 'hybrid_score')\n",
    "        :param top_k: 返回前 k 个结果\n",
    "        :param alpha: cross_encoder占比\n",
    "        :param beta: hybrid_score占比\n",
    "        :return: 重排序后的结果\n",
    "        \"\"\"\n",
    "        # 初始化中文 Cross-Encoder 模型\n",
    "        model = CrossEncoder(self.reranker)\n",
    "        \n",
    "        # 准备输入：查询和候选文档对\n",
    "        pairs = [[query, match['text']] for match in matches]\n",
    "        # print(f\"[DEBUG] Cross-Encoder input pairs: {pairs}\")\n",
    "        \n",
    "        # 计算相关性得分\n",
    "        scores = model.predict(pairs)\n",
    "        # print(f\"[DEBUG] Cross-Encoder scores: {scores}\")\n",
    "        \n",
    "        # 将得分添加到 matches 中\n",
    "        for match, score in zip(matches, scores):\n",
    "            match['cross_encoder_score'] = float(score)  # 转换为 float 以便排序\n",
    "        \n",
    "        # 按 Cross-Encoder 得分排序\n",
    "        reranked_matches = sorted(matches, key=lambda x: x['cross_encoder_score']*alpha + x['hybrid_score']*beta, reverse=True)\n",
    "        \n",
    "        print(reranked_matches[:top_k])\n",
    "        # 返回前 top_k 个结果\n",
    "        return reranked_matches[:top_k]\n",
    "    \n",
    "    # 将检索的结果处理为字符串上下文\n",
    "    def format_results(self, results, K):\n",
    "        response = []\n",
    "        if results:\n",
    "            graph_matches = [r for r in results if r['relation'] != 'vector_match']\n",
    "            if graph_matches:\n",
    "                response.append(\"检索结果:\")\n",
    "                for rel in graph_matches:\n",
    "                    # response.append(\n",
    "                    #     f\"• {rel['phenomenon']}-{rel['relation']}->{rel['target']}(相关度: {rel['score']:.3f})\")\n",
    "                    response.append(\n",
    "                        f\"• {rel['phenomenon']}-{rel['relation']}->{rel['target']}\")\n",
    "                    \n",
    "                response.append(\"\")\n",
    "            \n",
    "            vector_only = [r for r in results if r['relation'] == 'vector_only']\n",
    "            \n",
    "            if vector_only:\n",
    "                response.append(\"向量检索结果:\")\n",
    "                for rel in vector_only:\n",
    "                    response.append(f\"• {rel['phenomenon']}——缺少相关条目\")\n",
    "        \n",
    "        return \"\\n\".join(response[:min(K+1, len(response))])\n",
    "    \n",
    "    # 完整的检索功能\n",
    "    def retriever(self, query, k=4, K=15):\n",
    "        final_result = []\n",
    "        \n",
    "        matches = self.hybrid_retriever(query)\n",
    "        reranked_matches = self.rerank_with_cross_encoder(query, matches, top_k=k)\n",
    "        \n",
    "        with self.graph_manager.driver.session() as session:\n",
    "            for match in reranked_matches:\n",
    "                text = match['text']\n",
    "                score = match['cross_encoder_score']\n",
    "                \n",
    "                graph_results = session.run(\"\"\"\n",
    "                MATCH (p: 现象)-[r]->(n)\n",
    "                WHERE p.text contains $text\n",
    "                RETURN p.text as phenomenon, type(r) as relation, n.text as target\n",
    "                \"\"\", text=text)\n",
    "                \n",
    "                relationships = []\n",
    "                for record in graph_results:\n",
    "                    relationships.append({\n",
    "                        'phenomenon': record['phenomenon'],\n",
    "                        'relation': record['relation'],\n",
    "                        'target': record['target'],\n",
    "                        'score': score\n",
    "                    })\n",
    "                \n",
    "                if relationships:\n",
    "                    final_result.extend(relationships)\n",
    "                else:\n",
    "                    final_result.append({\n",
    "                        'phenomenon': text,\n",
    "                        'relation': 'vector_match',\n",
    "                        'target': 'N/A',\n",
    "                        'text': text,\n",
    "                        'score': score\n",
    "                    })\n",
    "        context = self.format_results(final_result, K)\n",
    "        print(\"[DEGUG] context:\" + context)\n",
    "        return context\n",
    "    \n",
    "    def generate_answer(self, query, context):\n",
    "        history_str = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, p, a in self.history]) if self.history else \"无\"\n",
    "        # 构造提示词\n",
    "        prompt = f\"\"\"\n",
    "        基于以下对话历史和检索到的相关信息回答问题：\n",
    "\n",
    "        对话历史：\n",
    "        {history_str}\n",
    "\n",
    "        当前检索到的信息：\n",
    "        {context}\n",
    "\n",
    "        问题：\n",
    "        {query}\n",
    "\n",
    "        请提供清晰简洁的答案。注意：\n",
    "        1. 如果检索到了信息，请整合信息中的原话,按照类别分条回答\n",
    "        2. 如果提供的信息和对话历史不足以回答问题，请直接说明\"根据现有信息无法完整回答该问题\"\n",
    "        3. 如果问题与对话历史相关，请结合历史信息回答\n",
    "\n",
    "        答案：\n",
    "        \"\"\"\n",
    "        \n",
    "        # 调用 LLM\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content.strip()\n",
    "        return answer\n",
    "        \n",
    "    def ask(self, query):\n",
    "        ## 多轮问询结合\n",
    "        # 拼接query\n",
    "        all_query = \",\".join(q for q,p,a in self.history) if self.history else \"\"\n",
    "        all_query += query\n",
    "        phenomenon = self.refine_query(query)\n",
    "        phenomenons = \",\".join(p for q,p,a in self.history) if self.history else \"\" \n",
    "        \n",
    "        context = self.retriever(phenomenons + phenomenon)\n",
    "        answer = self.generate_answer(all_query, context)\n",
    "        \n",
    "        self.history.append((query,phenomenon, answer))\n",
    "        if len(self.history) > 3:  # 限制历史长度，避免上下文过长\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def check_memory(self):\n",
    "        print(self.history)\n",
    "    \n",
    "\n",
    "graph_manager = Neo4jGraphManager(uri=os.environ[\"NEO4J_URI\"], user=os.environ[\"NEO4J_USERNAME\"],password=os.environ[\"NEO4J_PASSWORD\"])\n",
    "\n",
    "rag = RAGPipeline(graph_manager)\n",
    "query1 = \"荧光屏上没有回波是怎么回事\"\n",
    "answer = rag.ask(query1)\n",
    "print(answer)\n",
    "# query2 = \"且用氛灯放在天线当中检查时氛灯辉亮\"\n",
    "# answer = rag.ask(query2)\n",
    "# print(answer)\n",
    "# query3 = \"且没有混频晶体电流或者混频晶体电流很小\"\n",
    "# answer = rag.ask(query3)"
   ],
   "id": "6f03769ab4c7d380",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\86159\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Refined Query (via LLM): 荧光屏上没有回波\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.580 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] BM25 search matches found: 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮 (score: 4.749)\n",
      "[DEBUG] BM25 search matches found: 荧光屏上没有回波，且用氛灯放在天线当中检查时氛灯辉亮，且没有混频晶体电流或者混频晶体电流很小 (score: 3.550)\n",
      "[DEBUG] BM25 search matches found: 荧光屏上没有回波，且用氛灯放在天线当中检查时氛灯辉量，且混频晶体电流正常 (score: 3.355)\n",
      "[DEBUG] BM25 search matches found: 有些量程上没有扫描线 (score: 2.944)\n",
      "[DEBUG] BM25 search matches found: 荧光屏上图像不规则抖动 (score: 1.453)\n",
      "[DEBUG] Query embedding: [-0.028193472, -0.043359682, -0.011660766, -0.015169728, -0.0033936026]...(truncated for brevity)\n",
      "[DEBUG] Vector length: 1024\n",
      "[DEBUG] Vector search matches found: 荧光屏上图像不规则抖动(vector_score: 0.973)\n",
      "[DEBUG] Vector search matches found: 荧光屏上出现辉亮圆(vector_score: 0.972)\n",
      "[DEBUG] Vector search matches found: 荧光屏中心出现辉亮圆(vector_score: 0.943)\n",
      "[DEBUG] Vector search matches found: 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮(vector_score: 0.930)\n",
      "[DEBUG] Vector search matches found: 荧光屏上没有回波，且用氛灯放在天线当中检查时氛灯辉量，且混频晶体电流正常(vector_score: 0.927)\n",
      "[DEBUG] Vector result count: 5\n",
      "[{'text': '荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮', 'hybrid_score': 2.0756542484350913, 'cross_encoder_score': 0.9982165694236755}, {'text': '荧光屏上没有回波，且用氛灯放在天线当中检查时氛灯辉量，且混频晶体电流正常', 'hybrid_score': 1.6553952496267232, 'cross_encoder_score': 0.9980294108390808}, {'text': '荧光屏上没有回波，且用氛灯放在天线当中检查时氛灯辉亮，且没有混频晶体电流或者混频晶体电流很小', 'hybrid_score': 1.0649303968215438, 'cross_encoder_score': 0.9990812540054321}, {'text': '荧光屏上图像不规则抖动', 'hybrid_score': 1.1169031276821009, 'cross_encoder_score': 0.5667893290519714}]\n",
      "[DEGUG] context:检索结果:\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-处理方法->检查量程的扫描末级电源电压\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-涉及的部件->扫描末级\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-原因->在个别量程，扫描末级没有电源电压\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-处理方法->测量磁控管灯丝电压,插紧灯丝插座\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-涉及的部件->磁控管灯丝\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-原因->磁控管灯丝插头接触不良，或磁控管灯丝电压数值不对\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-处理方法->用静电高压表测量发射机中高压整流器的输出电压,如果没有高压输出,则检查高压变压器、整流硅柱和滤波电容的质量\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-涉及的部件->高压整流器\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-原因->高压整流器没有高压输出\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-处理方法->在预调脉冲幅度足够大和调制管各级电压正常的条件下,如果把氛灯检电器放在调制管阳极附近氛灯应辉亮,否则更换调制管。在更换调制管后应重新对调制管老炼\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-原因->调制管衰老\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-处理方法->检查调制管各级电压,特别是帘栅压,因为帘栅压降压电阻容易烧断。\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-涉及的部件->调制管\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-原因->调制管的工作状态不对\n",
      "• 荧光屏上没有回波，且没有磁控管电流，且发射机高压指示灯辉亮-处理方法->用脉冲电压表测量调制管栅极的预调脉冲幅度。如果预调脉冲幅度不够,则应检查预调器和触发脉冲形成级的工作状态和有关元件\n",
      "荧光屏上没有回波可能由以下几个原因导致：\n",
      "1. 在个别量程，扫描末级没有电源电压。需要检查量程的扫描末级电源电压。\n",
      "2. 磁控管灯丝插头接触不良或磁控管灯丝电压数值不对。建议测量磁控管灯丝电压并插紧灯丝插座。\n",
      "3. 高压整流器没有高压输出，需检查发射机中高压整流器的输出电压，并确认高压变压器、整流硅柱和滤波电容的质量。\n",
      "4. 调制管衰老或工作状态不对。建议检查调制管各级电压特别是帘栅压，并用脉冲电压表测量预调脉冲幅度，必要时更换并重新老炼调制管。\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T01:47:27.458838Z",
     "start_time": "2025-04-26T01:47:27.450641Z"
    }
   },
   "cell_type": "code",
   "source": "rag.check_memory()",
   "id": "91d690565d0df51b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('荧光屏上没有回波是怎么回事', '荧光屏上没有回波', '荧光屏上没有回波可能由以下几个原因导致：\\n1. 在个别量程，扫描末级没有电源电压。需要检查量程的扫描末级电源电压。\\n2. 磁控管灯丝插头接触不良或磁控管灯丝电压数值不对。建议测量磁控管灯丝电压并插紧灯丝插座。\\n3. 高压整流器没有高压输出，需检查发射机中高压整流器的输出电压，并确认高压变压器、整流硅柱和滤波电容的质量。\\n4. 调制管衰老或工作状态不对。建议检查调制管各级电压特别是帘栅压，并用脉冲电压表测量预调脉冲幅度，必要时更换并重新老炼调制管。')]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T01:47:27.490534Z",
     "start_time": "2025-04-26T01:47:27.476930Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cb2219dc2db80045",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
